### One-command install (works on $10 boards, Docker, native Rust binary, Termux â€” anywhere ZeroClaw runs)

```bash
curl -fsSL https://raw.githubusercontent.com/profbernardoj/zero-everclaw/main/setup.sh | bash
```

Then:

```bash
zeroclaw onboard          # only first time
zeroclaw service restart  # or zeroclaw agent / zeroclaw daemon / zeroclaw gateway
```

Your ZeroClaw instantly switches to **decentralized Morpheus inference** (GLM-5 heavy default, GLM-4.7-flash fast, Kimi K2.5, Qwen3-235b, etc.) via the local OpenAI-compatible proxy on `http://127.0.0.1:8083/v1`.

No Rust rebuilds. No trait changes. No API bills. Stake MOR once â†’ unlimited use forever. ZeroClawâ€™s <10 ms cold start and 8.8 MB binary stay untouched.

### Whatâ€™s in the repo (ready right now)

- `README.md` â€” full docs, edge-device tips, config examples, staking, troubleshooting
- `setup.sh` â€” installs EverClaw proxy + guardian into `~/.everclaw`, starts it as service (systemd/OpenRC/launchd), **safely patches** `~/.zeroclaw/config.toml`
- `config.patch.toml` â€” exact TOML snippet (merged without overwriting your existing settings)
- `workspace/skills/enable-morpheus/` â€” ZeroClaw skill (TOML manifest + SKILL.md) â€” say `/enable-morpheus` in any channel to toggle, check status, or reconfigure
- `tools-src/morpheus-status/` â€” tiny native Rust status tool (optional, `zeroclaw tool add morpheus-status`)

### What setup.sh does automatically

1. Clones & installs the battle-tested EverClaw proxy + guardian (Node.js sidecar, ~80 MB total)
2. Starts proxy on boot (systemd user service or launchd)
3. Runs `zeroclaw onboard` if needed
4. Patches `~/.zeroclaw/config.toml` with this block (preserves everything else, backs up original):

```toml
# === zero-everclaw auto-added ===
default_provider = "custom:http://127.0.0.1:8083/v1"
default_model = "glm-5"                    # change to glm-4.7-flash / kimi-k2.5 / qwen3 anytime
api_key = "sk-morpheus"                    # dummy â€” proxy ignores it

# Optional: add more models
[models.glm5]
provider = "custom:http://127.0.0.1:8083/v1"
model = "glm-5"

[models.flash]
provider = "custom:http://127.0.0.1:8083/v1"
model = "glm-4.7-flash"
```

5. Adds the `enable-morpheus` skill to `~/.zeroclaw/workspace/skills/` so you can control it from chat

### Staking (unlimited use â€” same as all EverClaw forks)

```bash
cd ~/.everclaw
node scripts/everclaw-wallet.mjs setup
node scripts/everclaw-wallet.mjs stake   # stake MOR, never spend it
```

### Test it

```bash
curl http://127.0.0.1:8083/health
zeroclaw status
zeroclaw agent -m "Say hello using Morpheus"
# or in any channel: /enable-morpheus
```

### Why this is perfect for ZeroClawâ€™s zero-overhead philosophy

- ZeroClaw = <5 MB RAM, <10 ms startup, trait-driven swappable providers
- EverClaw proxy = completely separate process (runs on host, not inside Rust binary)
- Together = still the lightest decentralized AI stack on the planet
- Keeps sandboxing, allowlists, Docker runtime, hybrid memory, 70+ channels 100% intact
- Works with native, Docker, systemd, OpenRC â€” no friction

Star the repo, run the setup, and your ZeroClaw on a $10 RISC-V board now has **unlimited GLM-5-class intelligence** with zero recurring cost and full autonomy.

Letâ€™s put decentralized inference on every ZeroClaw deployment! ðŸ¦žðŸ”¥ðŸ¦€
